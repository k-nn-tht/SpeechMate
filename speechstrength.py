# -*- coding: utf-8 -*-
"""speechstrength.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LzKzs5VYyGEBvIXSgrIRt_LRhuzJXZbJ
"""

!pip install datasets

# dependencies
import pandas as pd
import numpy as np
from datasets import Dataset
import random
import time
import datetime
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler,random_split
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

import transformers
from transformers import BertForSequenceClassification, BertConfig,BertTokenizer,get_linear_schedule_with_warmup
from torch.optim import AdamW

# modify DebateSum dataset
df = pd.read_csv("DebateSumV3.csv", nrows = 100000)
df = df[["Full-Document"]]

output_filename = "modified_debatesum.csv"
df.to_csv(output_filename, index=False)

from google.colab import files
files.download(output_filename)

# split each document into sentences
import nltk
from nltk.tokenize import sent_tokenize
import string

df = pd.read_csv("modified_debatesum.csv")

nltk.download('punkt_tab')

split_df = pd.DataFrame(columns=['sentence'])

# split each row of modified debatesum into sentence entries
sentences = []
for document in df["Full-Document"]:
    sentence = sent_tokenize(document)
    sentences.extend(sentence)

split_df = pd.DataFrame({'sentence': sentences})

# normalize sentences
for i, sentence in enumerate(split_df["sentence"]):
    sentence = sentence.lower()
    sentence = sentence.translate(str.maketrans('', '', string.punctuation))
    split_df.loc[i, "sentence"] = " ".join(sentence.split()) # remove extra whitespace

# good_sentences =split_df.sample(n=50000)
# output_filename = "good_debatesum.csv"
# split_df.to_csv(output_filename, index=False)
# files.download(output_filename)

"""Start here for training on normalized data"""

# load csv
df = pd.read_csv("scored_sentences.csv")

df.shape

df['len'] = df['sentence'].str.len()
maxlen = df['len'].max()
print(maxlen)
df = df.drop('len', axis=1)

# remove NaN and inf
df = df.dropna()
df = df[df["score"].apply(lambda x: np.isfinite(x))]

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# turn df into lists
texts = [str(text) for text in df["sentence"]]
labels = [int(label) for label in df["score"]]

# tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

print(' Original: ', texts[0])

# Print the sentence split into tokens.
print('Tokenized: ', tokenizer.tokenize(texts[0]))

# Print the sentence mapped to token ids.
print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(texts[0])))

for text in texts:
    # tokenize text and add `[CLS]` and `[SEP]` tokens
    input_ids = tokenizer.encode(text, max_length = 512, truncation = True, add_special_tokens = True)

input_ids = []
attention_masks = []

for text in texts:
    # (1) tokenize sentence
    # (2) add `[CLS]` token to the start
    # (3) add the `[SEP]` token to the end
    # (4) map tokens to their IDs
    # (5) pad/truncate to 512
    # (6) create attention masks for [PAD] tokens
    encoded_dict = tokenizer.encode_plus(
                        text,                          # encoding sentence
                        add_special_tokens = True,     # add '[CLS]' and '[SEP]'
                        max_length = 512,          # pad/truncate sentence
                        pad_to_max_length = True,
                        return_attention_mask = True,   # construct attention mask
                        return_tensors = 'pt',     # return pytorch tensor
                   )

    # add encoded sentence to list
    input_ids.append(encoded_dict['input_ids'])

    # add attention mask to list
    attention_masks.append(encoded_dict['attention_mask'])

# Convert the lists into tensors.
input_ids = torch.cat(input_ids, dim = 0)
attention_masks = torch.cat(attention_masks, dim = 0)
labels = torch.tensor(labels)

# Print sentence 0, now as a list of IDs.
print('Original: ', texts[0])
print('Token IDs:', input_ids[0])

# combine the training inputs into a TensorDataset
dataset = TensorDataset(input_ids, attention_masks, labels)

# 80-30 train-test split

train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size

train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

# dataloader
batch_size = 16

# train dataloader, batching
train_dataloader = DataLoader(
            train_dataset,
            sampler = RandomSampler(train_dataset),
            batch_size = batch_size
        )

# read sequentially in validation set
validation_dataloader = DataLoader(
            val_dataset,
            sampler = SequentialSampler(val_dataset),
            batch_size = batch_size
        )

# load pertrained BERT model, add single linear classification layer
model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels = 11,
    output_attentions = False,
    output_hidden_states = False,
)

model = model.to(device)

optimizer = torch.optim.AdamW(model.parameters(),
                  lr = 2e-5,
                  eps = 1e-8
                )

# train on 2-4 epochs, adjust based on accuracy and overfitting
epochs = 3

# training step = # batches * # epochs
total_steps = len(train_dataloader) * epochs

# learning rate scheduler
scheduler = get_linear_schedule_with_warmup(optimizer,
                                            num_warmup_steps = 0,
                                            num_training_steps = total_steps)

# function to calculate the accuracy of our predictions vs labels
def flat_accuracy(preds, labels):
    pred_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    return np.sum(abs(pred_flat - labels_flat) <= 2) / len(labels_flat)

def format_time(elapsed):
    # convert time in seconds and returns a string hh:mm:ss
    # to nearest second
    elapsed_rounded = int(round((elapsed)))
    # format as hh:mm:ss
    return str(datetime.timedelta(seconds=elapsed_rounded))

import os

# For reproducibility
seed_val = 42
random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

training_stats = []
total_t0 = time.time()

best_eval_accuracy = 0  # move this outside the loop

for epoch_i in range(epochs):
    print('epoch {}'.format(epoch_i + 1))
    t0 = time.time()
    total_train_loss = 0
    model.train()

    for step, batch in enumerate(train_dataloader):
        b_input_ids = batch[0].to(device)
        b_input_mask = batch[1].to(device)
        b_labels = batch[2].to(device)

        optimizer.zero_grad()

        output = model(
            b_input_ids,
            token_type_ids=None,
            attention_mask=b_input_mask,
            labels=b_labels
        )

        loss = output.loss
        total_train_loss += loss.item()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()

    avg_train_loss = total_train_loss / len(train_dataloader)
    training_time = format_time(time.time() - t0)
    print("\n  avg training loss: {0:.2f}".format(avg_train_loss))
    print("  epoch time: {:}".format(training_time))

    # Validation
    t0 = time.time()
    model.eval()
    total_eval_accuracy = 0
    total_eval_loss = 0

    for batch in validation_dataloader:
        b_input_ids = batch[0].to(device)
        b_input_mask = batch[1].to(device)
        b_labels = batch[2].to(device)

        with torch.no_grad():
            output = model(
                b_input_ids,
                token_type_ids=None,
                attention_mask=b_input_mask,
                labels=b_labels
            )

        loss = output.loss
        total_eval_loss += loss.item()
        logits = output.logits.detach().cpu().numpy()
        label_ids = b_labels.to('cpu').numpy()
        total_eval_accuracy += flat_accuracy(logits, label_ids)

    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)
    avg_val_loss = total_eval_loss / len(validation_dataloader)
    validation_time = format_time(time.time() - t0)

    print("  Accuracy: {0:.2f}".format(avg_val_accuracy))

    # save best model in hugging face format
    if avg_val_accuracy > best_eval_accuracy:
        save_path = f'./debate_score_model_epoch_{epoch_i + 1}'
        model.save_pretrained(save_path)
        tokenizer.save_pretrained(save_path)
        print(f"  ðŸ”¥ Saved best model to {save_path}")
        best_eval_accuracy = avg_val_accuracy

    training_stats.append(
        {
            'epoch': epoch_i + 1,
            'Training Loss': avg_train_loss,
            'Valid. Loss': avg_val_loss,
            'Valid. Accur.': avg_val_accuracy,
            'Training Time': training_time,
            'Validation Time': validation_time
        }
    )

print("Total training time {:} (h:mm:ss)".format(format_time(time.time() - total_t0)))

"""Testing on new data"""

import pandas as pd
import numpy as np
import nltk
from nltk.tokenize import sent_tokenize
import string
import torch
from transformers import BertTokenizer, BertForSequenceClassification

# load saved model weights
model = BertForSequenceClassification.from_pretrained('/content/speech_score_model')
tokenizer = BertTokenizer.from_pretrained('/content/speech_score_model')

nltk.download('punkt_tab')

# preprocess new document
document = "The quick brown fox jumps over the lazy dog. Hello world! This is a test sentence for BERT. Um, I think that this, uh, is wrong, uh, yeah. Uh, uh uh uh u huh uh uh."
#document = "uh um i think yeah because it seems that uh huh"
sentences = sent_tokenize(document)

for i, sentence in enumerate(sentences):
    sentence = sentence.lower()
    sentence = sentence.translate(str.maketrans('', '', string.punctuation))
    sentences[i] = " ".join(sentence.split())

# tokenization
input_ids = []
attention_masks = []

for sentence in sentences:
    encoded_dict = tokenizer.encode_plus(
        sentence,
        add_special_tokens = True,
        max_length = 512,
        padding = 'max_length',
        truncation = True,
        return_attention_mask = True,
        return_tensors = 'pt'
    )
    input_ids.append(encoded_dict['input_ids'])
    attention_masks.append(encoded_dict['attention_mask'])

input_ids = torch.cat(input_ids, dim = 0)
attention_masks = torch.cat(attention_masks, dim = 0)

# predict score
model.eval()
with torch.no_grad():
    outputs = model(input_ids = input_ids, attention_mask = attention_masks)
    logits = outputs.logits
    predicted_scores = torch.argmax(logits, dim = 1)

# model.eval()
# with torch.no_grad():
#     out = model(**sample_input)
#     print(torch.argmax(out.logits, dim=1))

print("Predicted scores:", predicted_scores)

sample_input = tokenizer("uh um i think yeah because it seems that uh huh", return_tensors="pt")
model.eval()
with torch.no_grad():
    out = model(**sample_input)
    print(torch.argmax(out.logits, dim=1))

pip install assemblyai

import assemblyai as aai

aai.settings.api_key = ""

# You can use a local filepath:
# audio_file = "./example.mp3"

# Or use a publicly-accessible URL:
audio_file = (
    "https://assembly.ai/wildfires.mp3"
)

transcriber = aai.Transcriber()

transcript = transcriber.transcribe(audio_file)

if transcript.status == aai.TranscriptStatus.error:
    print("Transcription failed")

transcript.text